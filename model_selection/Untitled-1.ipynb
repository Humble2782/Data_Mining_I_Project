{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "963c81f4",
   "metadata": {},
   "source": [
    "# Improved Expert Model: Differentiating Class 1 vs 2\n",
    "This notebook builds an improved expert model focused on distinguishing class 1 (injured) vs class 2 (heavily injured), then integrates it into the existing hierarchical pipeline (stage 1: injury vs no injury).  \n",
    "Run this after the base data preparation notebook so that `X_train`, `X_test`, `y_train`, `y_test`, `cat_cols`, `num_cols` already exist."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "750afc75",
   "metadata": {},
   "source": [
    "## 1. Prepare 1-vs-2 expert dataset\n",
    "Filter training/test sets to rows with y ∈ {1,2}.  \n",
    "Binary encoding: y_bin = (y == 2).  \n",
    "Create stratified train/validation split for expert tuning, leaving the test subset only for final evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d68b8a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Safety checks (in case notebook is run standalone)\n",
    "required_vars = ['X_train','X_test','y_train','y_test','cat_cols','num_cols']\n",
    "missing = [v for v in required_vars if v not in globals()]\n",
    "if missing:\n",
    "    raise RuntimeError(f\"Missing required variables from previous preparation notebook: {missing}\")\n",
    "\n",
    "# Filter to classes 1 and 2\n",
    "mask_train_12 = y_train.isin([1,2])\n",
    "mask_test_12  = y_test.isin([1,2])\n",
    "\n",
    "X_train_1_2 = X_train[mask_train_12].copy()\n",
    "y_train_1_2 = y_train[mask_train_12].copy()\n",
    "\n",
    "X_test_1_2  = X_test[mask_test_12].copy()\n",
    "y_test_1_2  = y_test[mask_test_12].copy()\n",
    "\n",
    "# Binary label: 1 => class 2 (heavily injured), 0 => class 1 (injured)\n",
    "y_train_bin = (y_train_1_2 == 2).astype(int)\n",
    "\n",
    "# Stratified split for validation\n",
    "X_tr_1_2, X_val_1_2, y_tr_bin, y_val_bin = train_test_split(\n",
    "    X_train_1_2, y_train_bin, test_size=0.2, stratify=y_train_bin, random_state=42\n",
    ")\n",
    "\n",
    "cat_idx = [X_train.columns.get_loc(c) for c in X_train.columns if c not in num_cols]\n",
    "\n",
    "print(\"Train 1-2 shape:\", X_tr_1_2.shape, \"Validation shape:\", X_val_1_2.shape, \"Test subset shape:\", X_test_1_2.shape)\n",
    "print(\"Class balance (train_bin):\", np.bincount(y_tr_bin))\n",
    "print(\"Class balance (val_bin):  \", np.bincount(y_val_bin))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67d282d8",
   "metadata": {},
   "source": [
    "## 2. CatBoost expert (binary 1 vs 2): strong baseline with early stopping\n",
    "Use CatBoost with automatic class weights, early stopping, and compute validation F1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec94543f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import CatBoostClassifier\n",
    "\n",
    "cat_expert = CatBoostClassifier(\n",
    "    loss_function='Logloss',\n",
    "    eval_metric='F1',\n",
    "    task_type='GPU',\n",
    "    devices='0',\n",
    "    auto_class_weights='Balanced',\n",
    "    cat_features=cat_idx,\n",
    "    random_state=42,\n",
    "    iterations=10000,\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "cat_expert.fit(\n",
    "    X_tr_1_2, y_tr_bin,\n",
    "    eval_set=(X_val_1_2, y_val_bin),\n",
    "    use_best_model=True,\n",
    "    early_stopping_rounds=200\n",
    ")\n",
    "\n",
    "# Validation performance\n",
    "val_pred_bin = (cat_expert.predict_proba(X_val_1_2)[:,1] >= 0.5).astype(int)\n",
    "val_f1 = f1_score(y_val_bin, val_pred_bin, average='binary')\n",
    "print(f\"Baseline CatBoost (binary) validation F1: {val_f1:.4f}; best_iteration={cat_expert.get_best_iteration()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c30024b",
   "metadata": {},
   "source": [
    "## 3. Manual randomized search for CatBoost hyperparameters\n",
    "Sample ~30 configurations; keep the best by validation F1 (threshold=0.5).  \n",
    "Search spaces:\n",
    "- depth ∈ {6..10}\n",
    "- learning_rate ∈ logspace(-3, -0.5)\n",
    "- l2_leaf_reg ∈ logspace(-2, 2)\n",
    "- subsample ∈ [0.6, 0.9]\n",
    "- rsm ∈ [0.6, 1.0]\n",
    "- grow_policy ∈ {'SymmetricTree','Lossguide'}\n",
    "- border_count ∈ {128, 254}\n",
    "Iterations large (10000) with early stopping to truncate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6680356",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from math import log10\n",
    "\n",
    "def sample_hparams(n=30, rng=None):\n",
    "    rng = rng or np.random.default_rng(42)\n",
    "    configs = []\n",
    "    for _ in range(n):\n",
    "        cfg = {\n",
    "            'depth': int(rng.integers(6, 11)),\n",
    "            'learning_rate': 10 ** rng.uniform(-3.0, -0.5),\n",
    "            'l2_leaf_reg': 10 ** rng.uniform(-2.0, 2.0),\n",
    "            'subsample': rng.uniform(0.6, 0.9),\n",
    "            'rsm': rng.uniform(0.6, 1.0),\n",
    "            'grow_policy': random.choice(['SymmetricTree','Lossguide']),\n",
    "            'border_count': int(random.choice([128, 254]))\n",
    "        }\n",
    "        configs.append(cfg)\n",
    "    return configs\n",
    "\n",
    "search_space = sample_hparams(32)\n",
    "\n",
    "best_cat_expert = cat_expert\n",
    "best_f1 = val_f1\n",
    "best_cfg = {\n",
    "    'depth': cat_expert.get_param('depth'),\n",
    "    'learning_rate': cat_expert.get_param('learning_rate')\n",
    "}\n",
    "\n",
    "for i, cfg in enumerate(search_space, 1):\n",
    "    model = CatBoostClassifier(\n",
    "        loss_function='Logloss',\n",
    "        eval_metric='F1',\n",
    "        task_type='GPU',\n",
    "        devices='0',\n",
    "        auto_class_weights='Balanced',\n",
    "        cat_features=cat_idx,\n",
    "        iterations=10000,\n",
    "        random_state=42 + i,\n",
    "        verbose=False,\n",
    "        early_stopping_rounds=200,\n",
    "        **cfg\n",
    "    )\n",
    "    model.fit(\n",
    "        X_tr_1_2, y_tr_bin,\n",
    "        eval_set=(X_val_1_2, y_val_bin),\n",
    "        use_best_model=True\n",
    "    )\n",
    "    preds_val = (model.predict_proba(X_val_1_2)[:,1] >= 0.5).astype(int)\n",
    "    f1v = f1_score(y_val_bin, preds_val)\n",
    "    if f1v > best_f1:\n",
    "        best_f1 = f1v\n",
    "        best_cat_expert = model\n",
    "        best_cfg = cfg\n",
    "    if i % 5 == 0:\n",
    "        print(f\"[{i}/{len(search_space)}] current best F1={best_f1:.4f}\")\n",
    "\n",
    "print(\"Best CatBoost expert validation F1:\", best_f1)\n",
    "print(\"Best hyperparameters:\", best_cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1c6f67a",
   "metadata": {},
   "source": [
    "## 4. Probability calibration (isotonic) for the expert\n",
    "Calibrate predicted probabilities of class 2 (heavily injured) on validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e477a476",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.isotonic import IsotonicRegression\n",
    "\n",
    "p_val_cat_raw = best_cat_expert.predict_proba(X_val_1_2)[:,1]\n",
    "iso_cat = IsotonicRegression(y_min=0.0, y_max=1.0, increasing=True, out_of_bounds='clip')\n",
    "iso_cat.fit(p_val_cat_raw, y_val_bin)\n",
    "\n",
    "def cat_expert_proba_calibrated(X):\n",
    "    p = best_cat_expert.predict_proba(X)[:,1]\n",
    "    return iso_cat.predict(p)\n",
    "\n",
    "# Quick check\n",
    "raw_f1 = f1_score(y_val_bin, (p_val_cat_raw >= 0.5).astype(int))\n",
    "cal_f1 = f1_score(y_val_bin, (iso_cat.predict(p_val_cat_raw) >= 0.5).astype(int))\n",
    "print(f\"CatBoost raw val F1 @0.5: {raw_f1:.4f} | calibrated F1 @0.5: {cal_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ac5ed5c",
   "metadata": {},
   "source": [
    "## 5. HGB expert (binary 1 vs 2) with ordinal encoding\n",
    "Train several HistGradientBoostingClassifier configurations, pick best by validation F1, then calibrate probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8997da65",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.utils.class_weight import compute_sample_weight\n",
    "\n",
    "hgb_transformer = ColumnTransformer([\n",
    "    ('cat', OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1), cat_cols),\n",
    "    ('num', 'passthrough', num_cols)\n",
    "], remainder='drop')\n",
    "\n",
    "X_tr_hgb = hgb_transformer.fit_transform(X_tr_1_2)\n",
    "X_val_hgb = hgb_transformer.transform(X_val_1_2)\n",
    "X_te_hgb  = hgb_transformer.transform(X_test_1_2)\n",
    "\n",
    "cat_feature_indices = list(range(len(cat_cols)))\n",
    "sw_tr = compute_sample_weight(class_weight='balanced', y=y_tr_bin)\n",
    "\n",
    "# Hyperparameter grid (compact)\n",
    "learning_rates = [0.03, 0.05, 0.08]\n",
    "max_iters = [500, 800, 1100]\n",
    "max_depths = [6, 8, None]\n",
    "min_leafs = [30, 60, 120]\n",
    "l2_regs = np.logspace(-3, 2, 5)\n",
    "\n",
    "best_hgb = None\n",
    "best_hgb_f1 = -1\n",
    "hgb_results = []\n",
    "\n",
    "for lr in learning_rates:\n",
    "    for mi in max_iters:\n",
    "        for md in max_depths:\n",
    "            for ml in min_leafs:\n",
    "                for reg in l2_regs:\n",
    "                    clf = HistGradientBoostingClassifier(\n",
    "                        categorical_features=cat_feature_indices,\n",
    "                        learning_rate=lr,\n",
    "                        max_iter=mi,\n",
    "                        max_depth=md,\n",
    "                        min_samples_leaf=ml,\n",
    "                        l2_regularization=reg,\n",
    "                        early_stopping='auto',\n",
    "                        random_state=42\n",
    "                    )\n",
    "                    clf.fit(X_tr_hgb, y_tr_bin, sample_weight=sw_tr)\n",
    "                    pred_val = (clf.predict_proba(X_val_hgb)[:,1] >= 0.5).astype(int)\n",
    "                    f1v = f1_score(y_val_bin, pred_val)\n",
    "                    hgb_results.append((f1v, lr, mi, md, ml, reg))\n",
    "                    if f1v > best_hgb_f1:\n",
    "                        best_hgb_f1 = f1v\n",
    "                        best_hgb = clf\n",
    "\n",
    "print(f\"Best HGB validation F1 @0.5: {best_hgb_f1:.4f}\")\n",
    "\n",
    "# Calibrate HGB\n",
    "p_val_hgb_raw = best_hgb.predict_proba(X_val_hgb)[:,1]\n",
    "iso_hgb = IsotonicRegression(y_min=0.0, y_max=1.0, increasing=True, out_of_bounds='clip')\n",
    "iso_hgb.fit(p_val_hgb_raw, y_val_bin)\n",
    "\n",
    "def hgb_expert_proba_calibrated(X):\n",
    "    p = best_hgb.predict_proba(hgb_transformer.transform(X))[:,1] if X.shape[1] == X_tr_1_2.shape[1] else best_hgb.predict_proba(X)[:,1]\n",
    "    return iso_hgb.predict(p)\n",
    "\n",
    "raw_hgb_f1 = f1_score(y_val_bin, (p_val_hgb_raw >= 0.5).astype(int))\n",
    "cal_hgb_f1 = f1_score(y_val_bin, (iso_hgb.predict(p_val_hgb_raw) >= 0.5).astype(int))\n",
    "print(f\"HGB raw val F1 @0.5: {raw_hgb_f1:.4f} | calibrated F1 @0.5: {cal_hgb_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e58fea5",
   "metadata": {},
   "source": [
    "## 6. Soft-voting ensemble of CatBoost and HGB experts (tune ensemble weight)\n",
    "Compute calibrated probabilities and search combination weights w ∈ [0.1,0.9] and coarse thresholds τ to maximize macro F1 on labels {1,2}."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "216283b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calibrated validation probabilities\n",
    "P_cat_val = cat_expert_proba_calibrated(X_val_1_2)\n",
    "P_hgb_val = hgb_expert_proba_calibrated(X_val_1_2)\n",
    "\n",
    "weight_grid = np.arange(0.1, 0.91, 0.05)\n",
    "threshold_coarse = np.arange(0.3, 0.81, 0.05)\n",
    "\n",
    "best_ens = {'f1': -1}\n",
    "\n",
    "def macro_f1_12(y_true_bin, y_pred_bin):\n",
    "    # Map back to classes {1,2} for macro F1\n",
    "    y_true_full = np.where(y_true_bin == 1, 2, 1)\n",
    "    y_pred_full = np.where(y_pred_bin == 1, 2, 1)\n",
    "    return f1_score(y_true_full, y_pred_full, average='macro')\n",
    "\n",
    "for w in weight_grid:\n",
    "    P_comb = w * P_hgb_val + (1 - w) * P_cat_val\n",
    "    for tau in threshold_coarse:\n",
    "        pred_bin = (P_comb >= tau).astype(int)\n",
    "        f1m = macro_f1_12(y_val_bin, pred_bin)\n",
    "        if f1m > best_ens['f1']:\n",
    "            best_ens = {'f1': f1m, 'w': w, 'tau': float(tau)}\n",
    "\n",
    "print(\"Best ensemble (coarse search):\", best_ens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c7a6f40",
   "metadata": {},
   "source": [
    "## 7. Tune decision threshold (refinement) for class 2\n",
    "Refine threshold τ with finer grid for the chosen ensemble weight; evaluate macro F1 over {1,2}."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee954b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "fine_thresholds = np.linspace(max(0.05, best_ens['tau'] - 0.15),\n",
    "                              min(0.95, best_ens['tau'] + 0.15), 61)\n",
    "w_best = best_ens['w']\n",
    "\n",
    "best_refined = {'f1': -1, 'tau': best_ens['tau']}\n",
    "P_comb_val = w_best * P_hgb_val + (1 - w_best) * P_cat_val\n",
    "\n",
    "for tau in fine_thresholds:\n",
    "    pred_bin = (P_comb_val >= tau).astype(int)\n",
    "    f1m = macro_f1_12(y_val_bin, pred_bin)\n",
    "    if f1m > best_refined['f1']:\n",
    "        best_refined = {'f1': f1m, 'tau': float(tau)}\n",
    "\n",
    "print(\"Refined ensemble threshold:\", best_refined)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "716190d6",
   "metadata": {},
   "source": [
    "## 8. Integrate expert into hierarchical pipeline\n",
    "Reuse (or train if absent) a stage-1 model for (0 vs 1+2) to obtain p_inj.  \n",
    "For p_heavy_cond (class 2 vs 1 among injured), use calibrated ensemble probability.  \n",
    "Combine:\n",
    "- p2 = p_inj * p_heavy_cond  \n",
    "- p0 = 1 - p_inj  \n",
    "- p1 = p_inj - p2  \n",
    "Tune injury threshold t1 over a grid; heavy threshold = refined τ.  \n",
    "Decision rule:\n",
    "- if p_heavy_cond ≥ τ → class 2  \n",
    "- elif p_inj ≥ t1 → class 1  \n",
    "- else → class 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8adfad5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.utils.class_weight import compute_sample_weight\n",
    "\n",
    "# Attempt to reuse stage1_clf; if missing, train a new one\n",
    "if 'stage1_clf' not in globals():\n",
    "    print(\"stage1_clf not found. Training a new stage-1 classifier (0 vs 1+2)...\")\n",
    "    stage1_y = (y_train >= 1).astype(int)\n",
    "    stage1_sw = compute_sample_weight(class_weight='balanced', y=stage1_y)\n",
    "    stage1_transformer = ColumnTransformer([\n",
    "        ('cat', OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1), cat_cols),\n",
    "        ('num', 'passthrough', num_cols)\n",
    "    ], remainder='drop')\n",
    "    X_tr_stage1 = stage1_transformer.fit_transform(X_train)\n",
    "    cat_idx_stage1 = list(range(len(cat_cols)))\n",
    "    stage1_clf = HistGradientBoostingClassifier(\n",
    "        categorical_features=cat_idx_stage1,\n",
    "        learning_rate=0.05,\n",
    "        max_depth=6,\n",
    "        max_iter=700,\n",
    "        min_samples_leaf=60,\n",
    "        l2_regularization=1.0,\n",
    "        random_state=42\n",
    "    )\n",
    "    stage1_clf.fit(X_tr_stage1, stage1_y, sample_weight=stage1_sw)\n",
    "    # Store transformer for later\n",
    "    hier_transformer = stage1_transformer\n",
    "else:\n",
    "    # If stage1 model exists, try to infer its transformer if present\n",
    "    if 'hier_transformer' not in globals():\n",
    "        print(\"Warning: 'hier_transformer' not found; reconstructing for inference.\")\n",
    "        hier_transformer = ColumnTransformer([\n",
    "            ('cat', OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1), cat_cols),\n",
    "            ('num', 'passthrough', num_cols)\n",
    "        ], remainder='drop')\n",
    "        hier_transformer.fit(X_train)\n",
    "\n",
    "# p_inj on test\n",
    "X_te_full = hier_transformer.transform(X_test)\n",
    "p_inj = stage1_clf.predict_proba(X_te_full)[:,1]  # P(y >= 1)\n",
    "\n",
    "# Calibrated expert ensemble probability for class 2 vs 1\n",
    "def ensemble_expert_proba(X):\n",
    "    # Use direct frames (expects same columns)\n",
    "    p_cat = cat_expert_proba_calibrated(X)\n",
    "    p_hgb = hgb_expert_proba_calibrated(X)\n",
    "    return w_best * p_hgb + (1 - w_best) * p_cat\n",
    "\n",
    "p_heavy_cond = ensemble_expert_proba(X_test)  # apply to all rows\n",
    "\n",
    "tau = best_refined['tau']\n",
    "\n",
    "# Combine hierarchical probabilities\n",
    "p2 = p_inj * p_heavy_cond\n",
    "p0 = 1.0 - p_inj\n",
    "p1 = p_inj - p2\n",
    "probs = np.vstack([p0, p1, p2]).T\n",
    "probs = np.clip(probs, 0, 1)\n",
    "probs /= probs.sum(axis=1, keepdims=True)\n",
    "\n",
    "# Tune t1 (injury threshold) while keeping tau fixed\n",
    "injury_thresholds = np.linspace(0.2, 0.9, 71)\n",
    "best_hier = {'macro_f1': -1}\n",
    "\n",
    "y_true = y_test.to_numpy()\n",
    "\n",
    "for t1 in injury_thresholds:\n",
    "    # Decision\n",
    "    y_pred = np.zeros_like(y_true)\n",
    "    heavy_mask = p_heavy_cond >= tau\n",
    "    inj_mask = p_inj >= t1\n",
    "    y_pred[inj_mask] = 1\n",
    "    y_pred[heavy_mask] = 2\n",
    "\n",
    "    mf1 = f1_score(y_true, y_pred, average='macro')\n",
    "    if mf1 > best_hier['macro_f1']:\n",
    "        best_hier = {'macro_f1': mf1, 't1': float(t1)}\n",
    "\n",
    "print(\"Best hierarchical injury threshold search:\", best_hier)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "425f756c",
   "metadata": {},
   "source": [
    "## 9. Final evaluation and plots\n",
    "Report:\n",
    "1. Expert performance on {1,2} test subset.\n",
    "2. Full hierarchical performance (all classes).\n",
    "Save artifacts (models, calibrators, parameters)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "965ecefc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import dump\n",
    "import os\n",
    "\n",
    "# Final expert evaluation on {1,2} subset\n",
    "P_cat_test = cat_expert_proba_calibrated(X_test_1_2)\n",
    "P_hgb_test = hgb_expert_proba_calibrated(X_test_1_2)\n",
    "P_comb_test = w_best * P_hgb_test + (1 - w_best) * P_cat_test\n",
    "y_pred_expert_bin = (P_comb_test >= tau).astype(int)\n",
    "y_pred_expert_full = np.where(y_pred_expert_bin==1, 2, 1)\n",
    "\n",
    "print(\"=== Expert (classes 1 vs 2) Test Evaluation ===\")\n",
    "print(classification_report(y_test_1_2, y_pred_expert_full, digits=4))\n",
    "ConfusionMatrixDisplay(\n",
    "    confusion_matrix(y_test_1_2, y_pred_expert_full, labels=[1,2], normalize='true'),\n",
    "    display_labels=['injured (1)','heavily injured (2)']\n",
    ").plot()\n",
    "plt.title(\"Expert Confusion Matrix (Normalized)\")\n",
    "plt.show()\n",
    "\n",
    "# Hierarchical predictions with best thresholds\n",
    "t1_best = best_hier['t1']\n",
    "y_pred_hier = np.zeros_like(y_true)\n",
    "heavy_mask = p_heavy_cond >= tau\n",
    "inj_mask = p_inj >= t1_best\n",
    "y_pred_hier[inj_mask] = 1\n",
    "y_pred_hier[heavy_mask] = 2\n",
    "\n",
    "print(\"=== Hierarchical Full Evaluation ===\")\n",
    "print(f\"Chosen thresholds: t1(injury)={t1_best:.3f}, tau(heavy)={tau:.3f}\")\n",
    "print(classification_report(y_true, y_pred_hier, digits=4))\n",
    "ConfusionMatrixDisplay(\n",
    "    confusion_matrix(y_true, y_pred_hier, labels=[0,1,2], normalize='true'),\n",
    "    display_labels=['uninjured (0)','injured (1)','heavily injured (2)']\n",
    ").plot()\n",
    "plt.title(\"Hierarchical Confusion Matrix (Normalized)\")\n",
    "plt.show()\n",
    "\n",
    "# Save artifacts\n",
    "artifacts_dir = \"expert_artifacts\"\n",
    "os.makedirs(artifacts_dir, exist_ok=True)\n",
    "\n",
    "dump(best_cat_expert, os.path.join(artifacts_dir, \"cat_expert_best.joblib\"))\n",
    "dump(iso_cat, os.path.join(artifacts_dir, \"iso_cat.joblib\"))\n",
    "dump(best_hgb, os.path.join(artifacts_dir, \"hgb_expert_best.joblib\"))\n",
    "dump(hgb_transformer, os.path.join(artifacts_dir, \"hgb_transformer.joblib\"))\n",
    "dump(iso_hgb, os.path.join(artifacts_dir, \"iso_hgb.joblib\"))\n",
    "dump(stage1_clf, os.path.join(artifacts_dir, \"stage1_clf.joblib\"))\n",
    "dump(hier_transformer, os.path.join(artifacts_dir, \"hier_transformer.joblib\"))\n",
    "\n",
    "config = {\n",
    "    'ensemble_weight_hgb': w_best,\n",
    "    'tau_heavy': tau,\n",
    "    't1_injury': t1_best,\n",
    "    'cat_expert_params': best_cfg,\n",
    "    'best_cat_val_f1': best_f1,\n",
    "    'best_hgb_val_f1': best_hgb_f1,\n",
    "    'refined_macro_f1_val_12': best_refined['f1'],\n",
    "    'hierarchical_macro_f1_test': f1_score(y_true, y_pred_hier, average='macro')\n",
    "}\n",
    "\n",
    "import json\n",
    "with open(os.path.join(artifacts_dir, \"config.json\"), \"w\") as f:\n",
    "    json.dump(config, f, indent=2)\n",
    "\n",
    "print(\"Artifacts saved to\", artifacts_dir)\n",
    "print(\"Configuration:\", config)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
